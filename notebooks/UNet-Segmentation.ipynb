{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append('../')\n",
    "\n",
    "import tensorflow as tf \n",
    "import numpy as np\n",
    "\n",
    "\n",
    "from MRIsegm.datagenerators import create_segmentation_generator\n",
    "from MRIsegm.metrics import dice_coef\n",
    "from MRIsegm.losses import DiceBCEloss, soft_dice_loss\n",
    "from MRIsegm.models import unet\n",
    "from MRIsegm.graphics import show_dataset, plot_history, show_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 666\n",
    "BATCH_SIZE_TRAIN = 4\n",
    "BATCH_SIZE_VALIDATION = 4\n",
    "\n",
    "IMAGE_HEIGHT = 128\n",
    "IMAGE_WIDTH =  128\n",
    "IMG_SIZE = (IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "\n",
    "NUM_OF_EPOCHS = 100\n",
    "\n",
    "NUM_TRAIN = 406\n",
    "NUM_VALIDATION = 80\n",
    "\n",
    "EPOCH_STEP_TRAIN = NUM_TRAIN // BATCH_SIZE_TRAIN\n",
    "EPOCH_STEP_VALIDATION = NUM_VALIDATION // BATCH_SIZE_VALIDATION\n",
    "\n",
    "data_dir_training = '../data/training'\n",
    "data_dir_train_img = os.path.join(data_dir_training, 'img')\n",
    "data_dir_train_mask = os.path.join(data_dir_training, 'mask')\n",
    "\n",
    "data_dir_validation = '../data/validation'\n",
    "data_dir_validation_img = os.path.join(data_dir_validation, 'img')\n",
    "data_dir_validation_mask = os.path.join(data_dir_validation, 'mask')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_gen_args_img = dict(rescale=1./255, rotation_range=5,horizontal_flip=True)\n",
    "data_gen_args_mask = dict(rescale=1./255, rotation_range=5,horizontal_flip=True)\n",
    "\n",
    "val_data_gen_args_img = dict(rescale=1./255)\n",
    "val_data_gen_args_mask = dict(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_generator = create_segmentation_generator(data_dir_train_img, data_dir_train_mask, BATCH_SIZE_TRAIN, IMG_SIZE, SEED, data_gen_args_img, data_gen_args_mask)\n",
    "\n",
    "validation_generator = create_segmentation_generator(data_dir_validation_img, data_dir_validation_mask, BATCH_SIZE_VALIDATION, IMG_SIZE, SEED, val_data_gen_args_img, val_data_gen_args_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show trainig data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_dataset(train_generator, 3) # training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_dataset(validation_generator, 3) # validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = unet(IMAGE_HEIGHT, IMAGE_WIDTH, n_levels=4, initial_features=32)\n",
    "\n",
    "\n",
    "\n",
    "optimizer = 'adam'\n",
    "loss = soft_dice_loss\n",
    "metrics = [  dice_coef ]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = model.name + f'_{IMAGE_HEIGHT}_{IMAGE_WIDTH}' \n",
    "\n",
    "\n",
    "if type(optimizer) == str: \n",
    "    model_name = model_name + f'_OPT={optimizer}' \n",
    "else:\n",
    "    model_name = model_name + f'_OPT={optimizer._name}'\n",
    "\n",
    "\n",
    "if type(loss) == str: \n",
    "    model_name = model_name + f'_LOSS={loss}' \n",
    "else:\n",
    "    model_name = model_name + f'_LOSS={loss.__name__}'\n",
    "\n",
    "print('model name: ', model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: logs_dir = '../data/models/logs/model_name'\n",
    "csv_dir = '../data/CSV/'\n",
    "\n",
    "callbacks = [\n",
    "                  tf.keras.callbacks.ModelCheckpoint('../data/models/checkpoints/' + model_name + '_checkpoint' + '.h5', save_best_only=True),\n",
    "                  tf.keras.callbacks.CSVLogger( csv_dir + model_name + '.csv', separator=',', append=False),\n",
    "                  tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss')\n",
    "                  # optional: tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
    "]\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "            steps_per_epoch=EPOCH_STEP_TRAIN, \n",
    "            validation_data=validation_generator, \n",
    "            validation_steps=EPOCH_STEP_VALIDATION,\n",
    "            epochs=NUM_OF_EPOCHS,\n",
    "            callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('../data/models/' + model_name + '.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optional: %load_ext tensorboard\n",
    "\n",
    "# optional: !tensorboard --logdir log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Evaluating on validation data\")\n",
    "evaluation = model.evaluate(validation_generator, batch_size=BATCH_SIZE_VALIDATION, steps=EPOCH_STEP_VALIDATION, return_dict=True)\n",
    "print(evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('../data/evals/' + model_name + '_eval.txt', 'w') as file:\n",
    "     file.write(json.dumps(evaluation))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(model_name, history, metrics, loss, custom_loss=True, custom_metrics=True, figsize=(18,8),labelsize=13, path='../data/plots/' + model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training images prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_prediction(datagen=train_generator, model=model , num=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation images prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_prediction(datagen=validation_generator, model=model , num=10)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9d93fbf5f19ccc2cddbe34694f7f49306bad9b2cdca2175ce5bcebb4e48816e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('tensorflow': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}