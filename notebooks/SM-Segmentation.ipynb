{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys, os\n",
    "sys.path.append('../')\n",
    "\n",
    "import tensorflow as tf \n",
    "import segmentation_models as sm\n",
    "\n",
    "from MRIsegm.datagenerators import create_segmentation_generator\n",
    "from MRIsegm.metrics import dice_coef\n",
    "from MRIsegm.losses import DiceBCEloss\n",
    "\n",
    "from MRIsegm.graphics import show_dataset, plot_history, show_prediction"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "BACKBONE = 'efficientnetb0'\n",
    "\n",
    "SEED = 666\n",
    "BATCH_SIZE_TRAIN = 8\n",
    "BATCH_SIZE_VALIDATION = 8\n",
    "\n",
    "IMAGE_HEIGHT = 256\n",
    "IMAGE_WIDTH =  256\n",
    "IMG_SIZE = (IMAGE_HEIGHT, IMAGE_WIDTH)\n",
    "\n",
    "NUM_OF_EPOCHS = 100\n",
    "\n",
    "NUM_TRAIN = 406\n",
    "NUM_VALIDATION = 80\n",
    "\n",
    "EPOCH_STEP_TRAIN = NUM_TRAIN // BATCH_SIZE_TRAIN\n",
    "EPOCH_STEP_VALIDATION = NUM_VALIDATION // BATCH_SIZE_VALIDATION\n",
    "\n",
    "data_dir_training = '../data/training'\n",
    "data_dir_train_img = os.path.join(data_dir_training, 'img')\n",
    "data_dir_train_mask = os.path.join(data_dir_training, 'mask')\n",
    "\n",
    "data_dir_validation = '../data/validation'\n",
    "data_dir_validation_img = os.path.join(data_dir_validation, 'img')\n",
    "data_dir_validation_mask = os.path.join(data_dir_validation, 'mask')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "data_gen_args_img = dict(rescale=1./255, rotation_range=5, horizontal_flip=True)\n",
    "data_gen_args_mask = dict(rescale=1./255, rotation_range=5, horizontal_flip=True)\n",
    "\n",
    "val_data_gen_args_img = dict(rescale=1./255)\n",
    "val_data_gen_args_mask = dict(rescale=1./255)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "train_generator = create_segmentation_generator(data_dir_train_img, data_dir_train_mask, BATCH_SIZE_TRAIN, IMG_SIZE, SEED, data_gen_args_img, data_gen_args_mask)\n",
    "\n",
    "validation_generator = create_segmentation_generator(data_dir_validation_img, data_dir_validation_mask, BATCH_SIZE_VALIDATION, IMG_SIZE, SEED, val_data_gen_args_img, val_data_gen_args_mask)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "show_dataset(train_generator, 3) # training"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model = sm.Unet(BACKBONE, input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 1), encoder_weights=None, activation='sigmoid')\n",
    "\n",
    "optimizer = 'Adam'\n",
    "# iou_loss = sm.losses.JaccardLoss(class_weights=None, class_indexes=None, per_image=False, smooth=1.)\n",
    "# BinaryFocalLoss = sm.losses.BinaryFocalLoss(alpha=0.25, gamma=2.0)\n",
    "loss = DiceBCEloss\n",
    "# iou_score = sm.metrics.IOUScore(smooth=1., name='iou_score')\n",
    "metrics = [  dice_coef ]\n",
    "\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "print(model.summary())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model_name = BACKBONE + f'_{IMAGE_HEIGHT}_{IMAGE_WIDTH}_BTC={BATCH_SIZE_TRAIN}_alpha3***' \n",
    "\n",
    "\n",
    "if type(optimizer) == str: \n",
    "    model_name = model_name + f'_OPT={optimizer}' \n",
    "else:\n",
    "    model_name = model_name + f'_OPT={optimizer._name}'\n",
    "\n",
    "\n",
    "if type(loss) == str: \n",
    "    model_name = model_name + f'_LOSS={loss}' \n",
    "else:\n",
    "    model_name = model_name + f'_LOSS={loss.__name__}'\n",
    "\n",
    "print('model name: ', model_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#optional: logs_dir = '../data/models/logs/' + model_name\n",
    "\n",
    "#optional: csv_dir = '../data/CSV/'\n",
    "\n",
    "callbacks = [\n",
    "                  #optional: tf.keras.callbacks.ModelCheckpoint('../data/models/checkpoints/' + model_name + '_checkpoint' + '.h5', save_best_only=True),\n",
    "                  #optional: tf.keras.callbacks.CSVLogger( csv_dir + model_name + '.csv', separator=',', append=False),\n",
    "                  tf.keras.callbacks.EarlyStopping(patience=10, monitor='val_loss')\n",
    "                  #optional: tf.keras.callbacks.TensorBoard(log_dir=logs_dir)\n",
    "]\n",
    "\n",
    "\n",
    "history = model.fit(train_generator,\n",
    "            steps_per_epoch=EPOCH_STEP_TRAIN, \n",
    "            validation_data=validation_generator, \n",
    "            validation_steps=EPOCH_STEP_VALIDATION,\n",
    "            epochs=NUM_OF_EPOCHS,\n",
    "            callbacks=callbacks)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.save('../data/models/' + model_name + '.h5')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#optional: %load_ext tensorboard\n",
    "\n",
    "#optional: !tensorboard --logdir log_dir"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "plot_history(model_name, history, metrics, loss, custom_loss=True, custom_metrics=True, figsize=(18,8),labelsize=13, path='../data/plots/' + model_name)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Evaluating on validation data\")\n",
    "evaluation = model.evaluate(validation_generator, batch_size=BATCH_SIZE_VALIDATION, steps=EPOCH_STEP_VALIDATION, return_dict=True)\n",
    "print(evaluation)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import json\n",
    "\n",
    "with open('../data/evals/' + model_name + '_eval.txt', 'w') as file:\n",
    "     file.write(json.dumps(evaluation))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "show_prediction(datagen=train_generator, model=model , num=5)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "show_prediction(datagen=validation_generator, model=model , num=15)"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f9d93fbf5f19ccc2cddbe34694f7f49306bad9b2cdca2175ce5bcebb4e48816e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.6 64-bit ('tensorflow': conda)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}